''' This function calculates KL divergence between two different distributions written in two text files
USAGE:
python KLdiv.py dfile1 dfile2 status

Example:
python KLdiv.py wFreqBLISS-d1.txt wFreqBLISS-d2.txt wfreq

STATUS: decides which function should be called
'wfreq': where the distributions are word frequencies, 'pfreq': distributions are pair frequencies

DFILE1: the first file containing the distribution of words/pairs
DFILE2: the second file containing another distribtuion of these words/pairs, prbably generated by another model
their format:
freq word
or about pairs:
freq word1 word2
'''

from math import log
import sys
import blissutil

ud = blissutil.UtilDict()

class Distribution:
	''' reads a file containing the frequency of random variables and returns a dictionary of corresponding probability distribution'''  

	def __init__(self,filename):
		self.filename = filename #the file containing the frequency distribution
		self.d = {} #the dictionary containing the probability distribution

	def get_distribution_words(self):
		'''gets a dictionary containing the words' distribution written in a file'''
		self.read_file_fq_words()
		self.d = normalize_d_values(self.d)
		return self.d
	def get_distribution_pairs(self):
		'''gets a dictionary containing the pairs' distribution written in a file'''
		self.read_file_fq_pairs()
		self.d = normalize_d_values(self.d)
		return self.d
	def read_file_fq_words(self):
		'''opens a file containing the frequency of words with the format
		frequency word
		and assigns it to a dictionary
		'''
		f = open(self.filename)
		for l in f.readlines():
			ws = l.split()
			fq = ws[0]
			w = ' '.join(ws[1:])
			#print 'fq:',fq,' w:',w		
			self.d[w.lower()]=float(fq) #a distribution corresponding to a language model
		f.close()
	#TODO: these read_file_fq_words/pair are the same!
	def read_file_fq_pairs(self):
		'''opens a file containing the frequency of pair of words with the format
		frequency word1 word2
		and assigns it to a dictionary
		'''
		f = open(self.filename)
		for l in f.readlines():
			ws = l.split()
			fq = ws[0]
			p = ' '.join(ws[1:])
			self.d[p.lower()]=float(fq) #a distribution corresponding to a language model
			#print 'd[p]:',p,' ',self.d[p]
		f.close()

	def synchronize_samplespace_add(self,DistributionReader):
		'''synchronizes its sample space, the keys of its distribution dictionary, with another distribution, adds random values of a distributions which are not shared by another, while sets the key equal to 0'''
		for w in self.d.keys():
			if w not in DistributionReader.d:
				DistributionReader.d[w]	= 0	
		for w in DistributionReader.d.keys():
			if w not in self.d:
				self.d[w] = 0
		check_same_alphabet(self.d.keys(), DistributionReader.d.keys())

	def synchronize_samplespace_del(self,DistributionReader):
		'''synchronizes its sample space, the keys of its distribution dictionary, with another distribution, removes random values of both distributions which are not shared by both'''
		for w in self.d.keys():
			if w not in DistributionReader.d:
				#print w,' ',self.d[w]
				del self.d[w]		
		#print '*********'
		for w in DistributionReader.d.keys():
			if w not in self.d:
				#print w,' ',DistributionReader.d[w]
				del DistributionReader.d[w]
		check_same_alphabet(self.d.keys(),DistributionReader.d.keys())

def log2(x):
	if x == 0:
		return 0
	return log(x,2)

def shannon_entropy(d):
	"return shannon entropy of random variable with probability distribution d1"

	d = normalize_d_values(d)
	return sum([-p*log2(p) for p in d.values()]) 

def shannon_entropy_filein(fn):
	
	"return shannon entropy of random variable with probability distribution d1"
	dr = Distribution(fn)
	d = dr.get_distribution_words()
	return shannon_entropy(d) 
 
def cross_entropy(d1,d2):
	'''return cross entropy of two random variables with probability distributions d1, d2'''
	check_same_alphabet(d1,d2)
	d1 = normalize_d_values(d1)
	d2 = normalize_d_values(d2)
	return sum([-d1[w]*log2(d2[w]) for w in d1])
 
def KL_divergence(d1,d2):
	'''returns Kullback-Leibler divergence of d2 from d1'''
	H_d1 = shannon_entropy(d1)
	H_d1_d2 = cross_entropy(d1,d2)
	return H_d1_d2 - H_d1

def mutual_inf(d):
	f_ = open('inf.out','a')
	f_.write("***********\n");
	'''returns mutual information of random variables in d.
	X={x1,x2}, Y={y1,y2}
	x1 y1 : fq1
	x2 y2 : fq2'''
	d1 = ud.split_key(d,n=0)
	d2 = ud.split_key(d,n=1)
	H_d1 = shannon_entropy(d1) #H(X)
	f_.write("H(X) "+str(H_d1)+"\n")
	H_d2 = shannon_entropy(d2) #H(Y)
	f_.write("H(Y) "+str(H_d2)+"\n")
	H_d  = shannon_entropy(d)  #H(X,Y)	
	f_.write("H(X,Y) "+str(H_d)+"\n")
        mtl_inf = H_d1 + H_d2 - H_d
	f_.write("*I(X,Y) "+str(mtl_inf)+"\n")
	return mtl_inf

def mutual_inf_filein(fn):
	dr = Distribution(fn)
	d = dr.get_distribution_pairs()
	return mutual_inf(d)    
    
def get_avg_dist(d1,d2):
	'''returns the average of d1 and d2 distributions'''
	avg_d = {}
	for w in d1:
		avg_d[w] = (d1[w] + d2[w])/2.0
	return avg_d
def read_sync_norm_pairdist(dr1,dr2):
	'''reads the frequency files and after synchronizing their sample spaces, normalize the frequencies'''
	#reading frequency files 
	dr1.read_file_fq_pairs()
	dr2.read_file_fq_pairs()
	
	#synchronizing sample spaces
	#dr1.synchronize_samplespace_add(dr2)
	dr1.synchronize_samplespace_del(dr2)
	#normalizing random values
	dr1.d = normalize_d_values(dr1.d)
	dr2.d = normalize_d_values(dr2.d)

def find_avg_dist(dr1, dr2):
	'''return the average distribution of two Distribution objects, calling two functions read_sync_norm_pairdist and get_avg_dist'''
	read_sync_norm_pairdist(dr1,dr2)
	return dr1.d, dr2.d
	#avg_d = get_avg_dist(dr1.d, dr2.d)
	#return dr1.d, avg_d

def normalize_d_values(d):
	"Normalize the values of a dictinary"
	s = float(sum(d.values()))
	for w in d:
		d[w] = d[w]/s				
	return d
def check_same_alphabet(k1, k2):
	if sorted(k1) != sorted(k2):
		raise Exception, "dictionaries don't still have the same probability space"


def main(f1,f2,status):
	#creating Distribution objects
	outfile.write('\n*INPUTS: ' + f1 +'\t'+ f2 +'\t'+ status +'\n')
	dr1=Distribution(f1)
	dr2=Distribution(f2)
	#TODO: why don't you do avg dist for word freqs?!!
	if status=='wfreq':
		d1 = dr1.get_distribution_words()
		d2 = dr2.get_distribution_words()
	elif status=='pfreq':
		d1, d2 = find_avg_dist(dr1,dr2)
	else:
		raise Exception('what is status?!!')
	s1 = shannon_entropy(d1)
	s2 = shannon_entropy(d2)
	kd = KL_divergence(d1,d2)
	outfile.write('SHANNON: '+ str(s1) +'\t'+ str(s2) +'\n')
	outfile.write('KL-d:    ' + str(kd) +'\n')

outfile = open('kld.o','a')
if __name__ == "__main__": 	
	if len(sys.argv)<4:
		raise Exception, "Missing input arguments!"
	f1 = sys.argv[1] #'wFreqBLISSsnt-sbj-18Mar.txt' #a file containing the frequency distribution of a language model
	f2 = sys.argv[2] #'wFreqBLISSsnt-nosem-18Mar.txt'
	status = sys.argv[3] #'wfreq' # 'wfreq': where the distributions are word frequencies, 'pfreq': distributions are pair frequencies
	print 'Writting to kld.o file...'
	main(f1,f2,status)
	f1, f2 = f2, f1
	main(f1,f2,status)
